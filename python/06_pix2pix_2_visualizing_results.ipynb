{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the Pix2Pix results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize some random results from the Pix2Pix model. To do so we just need to load a generator model and feed it with a \"source image\". Here you will need to provide an image that is compatible with the source you trained on. We will test some of the pre-trained models we provide you with. One transforming face landmarks into Rembrandt paintings, and one transforming face edges into comics drawings.\n",
    "\n",
    "This code expects the following directory structure:\n",
    "```bash\n",
    " models\n",
    " └── pix2pix_model_name\n",
    "      ├── model.1.pt\n",
    "      └── model.2.pt\n",
    "```\n",
    "\n",
    "You can find some [pre-trained models here](https://drive.google.com/drive/folders/19_Xfy10yOy1FCOu9Dn_ihiGct4oN0y__?usp=sharing), that are ready to be tested! Some of them have been trained by me, others by Irini and Daniel of [DMLAP](https://github.com/IriniKlz/DMLAP-2024/tree/main) (If you click on the top folder \"models\", you can download them all in one go.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "import torchvision as tv\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "import cv2\n",
    "from skimage import io\n",
    "from skimage import feature\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get cpu, gpu or mps device for training\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed directory structure -------------\n",
    "DATASETS_DIR = pathlib.Path(\"datasets\")\n",
    "DATASETS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "MODELS_DIR = pathlib.Path(\"models\")\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "GENERATED_DIR = pathlib.Path(\"generated\")\n",
    "GENERATED_DIR.mkdir(exist_ok=True)\n",
    "# ----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(model, image):\n",
    "    image = v2.ToImage()(image)\n",
    "\n",
    "    # Check for grayscale (single channel) or with alpha (four) (with GPT 4o)\n",
    "    if image.shape[0] == 1:\n",
    "        image = image.repeat(3, 1, 1)  # Convert grayscale to RGB by repeating the channel 3 times\n",
    "    elif image.shape[0] == 4:\n",
    "        image = image[:3, :, :]  # Remove the alpha channel by taking the first 3 channels (RGB)\n",
    "\n",
    "    image = v2.Resize((256,256), antialias=True)(image)\n",
    "    image = v2.ToDtype(torch.float32, scale=True)(image) # from [0,255] to [0,1]\n",
    "    image = image.to(device)\n",
    "    image = image[None, ...] # add a batch dimension\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image).detach().cpu()\n",
    "    output = outputs[0].permute(1, 2, 0) * 0.5 + 0.5\n",
    "    return output.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_bw_cv2(img):\n",
    "    \"\"\"Turn an image black and white using OpenCV\"\"\"\n",
    "    grey_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    return cv2.merge([grey_img, grey_img, grey_img]) # Force three channels for shape compat, thanks ChatGPT!\n",
    "\n",
    "def apply_canny_cv2(img, thresh1=160, thresh2=250, invert=False):\n",
    "    \"\"\"Apply the OpenCV Canny edge detector to an image\"\"\"\n",
    "    grey_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    edges = cv2.Canny(grey_img, thresh1, thresh2)\n",
    "    if invert:\n",
    "        edges = cv2.bitwise_not(edges)\n",
    "    return cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "def apply_canny_skimage(img, sigma=1.5, invert=False):\n",
    "    \"\"\"Apply the Scikit-Image Canny edge detector to an image\"\"\"\n",
    "    grey_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    edges = (feature.canny(grey_img, sigma=sigma)*255).astype(np.uint8)\n",
    "    if invert:\n",
    "        edges = cv2.bitwise_not(edges)\n",
    "    return cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model(models_dir, chosen_dir, chosen_model):\n",
    "    # check available directories\n",
    "    dnames = [d for d in models_dir.iterdir() if d.is_dir()]\n",
    "    \n",
    "    print(\"Available directories:\")\n",
    "    for i, d in enumerate(dnames):\n",
    "        print(f\"{i} | {d}\")\n",
    "    print()\n",
    "    \n",
    "    # choose your directory\n",
    "    d_id = chosen_dir\n",
    "\n",
    "    if d_id < 0 or d_id > len(dnames) - 1:\n",
    "        print(f\"!!Directory index ({chosen_dir}) invalid!!\")\n",
    "        return None\n",
    "    \n",
    "    chosen_dir = dnames[d_id]\n",
    "    \n",
    "    print(\"Chosen directory:\")\n",
    "    print(f\"{d_id} | {chosen_dir}\")\n",
    "    print(\"---\")\n",
    "    \n",
    "    # check available models\n",
    "    fnames = list(chosen_dir.glob(\"*.pt\"))\n",
    "    \n",
    "    if fnames:\n",
    "        print(\"Available models:\")\n",
    "        for i, f in enumerate(fnames):\n",
    "            print(f\"{i} | {f.name}\")\n",
    "        print()\n",
    "    \n",
    "        m_id = chosen_model\n",
    "\n",
    "        if m_id < 0 or m_id > len(fnames) - 1:\n",
    "            print(f\"!!Directory index ({chosen_model}) invalid!!\")\n",
    "            return None\n",
    "            \n",
    "        chosen_model = fnames[m_id]\n",
    "    \n",
    "        print(\"Chosen model:\")\n",
    "        print(f\"{m_id} | {chosen_model.name}\")\n",
    "        print(\"---\")\n",
    "        \n",
    "        return torch.jit.load(chosen_model,map_location=device)\n",
    "        \n",
    "    else:\n",
    "        print(f\"No model found in {chosen_dir}\")\n",
    "        print(\"---\")\n",
    "\n",
    "def display_transform(model, img_path, transform=None):\n",
    "    \n",
    "    img = io.imread(img_path)\n",
    "    \n",
    "    plt.figure(figsize=(12,12))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title('Input image')\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 3, 2)\n",
    "\n",
    "    # if we pass a transformation, preprocess the source image\n",
    "    if transform:\n",
    "        img = transform(img) # try apply_canny_cv2 & possible to play with channels \n",
    "        plt.title('Processed input')\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.subplot(1, 3, 3)\n",
    "    \n",
    "    img = generate(model, img)\n",
    "    plt.title('Generated image')\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated model loading & checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = load_model(MODELS_DIR, 1, 2)\n",
    "\n",
    "display_transform(model, \"images/spock256.jpg\", transform=apply_canny_skimage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = load_model(MODELS_DIR, 2, 0)\n",
    "\n",
    "if model:\n",
    "    display_transform(model, \"images/spock256.jpg\", transform=apply_bw_cv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facades "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try now with the \"facade\" dataset (one of the official pix2pix datasets).\n",
    "\n",
    "Let's load the model for a given epoch as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"facades\"\n",
    "PIX2PIX_DIR = MODELS_DIR / f\"pix2pix_{MODEL_NAME}\"\n",
    "\n",
    "# check available models\n",
    "fnames = list(PIX2PIX_DIR.glob(\"*.pt\"))\n",
    "\n",
    "for i, f in enumerate(fnames):\n",
    "    print(f\"{i} | {f}\")\n",
    "    \n",
    "if fnames:\n",
    "    ID = 0 # change me!\n",
    "\n",
    "    print()\n",
    "    print(f\"Loading:\")\n",
    "    print(f\"{ID} | {fnames[ID]}\")\n",
    "    model = torch.jit.load(fnames[ID],map_location=device)\n",
    "else:\n",
    "    print(f\"No model found in {PIX2PIX_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The color map for the facades is based on the [\"jet\" color map](https://matplotlib.org/stable/tutorials/colors/colormaps.html), which maps continuous values between 0 and 1 to a color palette. See [this document](https://cmp.felk.cvut.cz/~tylecr1/facade/CMP_facade_DB_2013.pdf) for details on what each color means. Matplotlib gives us easy access to these colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def jet_color(v):\n",
    "    c = matplotlib.colormaps.get_cmap('jet')\n",
    "    return np.array(c(v)) * 255 # The output of this function is between 0 and 1, we will use 0 to 255 colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use the Canvas API directly to generate some random rectangles filled with colors from the palette. The labels are (should be) organized as follows:  \n",
    "\n",
    "0. background\n",
    "1. facade\n",
    "2. window\n",
    "3. door\n",
    "4. cornice\n",
    "5. sill\n",
    "6. balcony\n",
    "7. blind\n",
    "8. deco\n",
    "9. molding\n",
    "10. pillar\n",
    "11. shop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import canvas\n",
    "\n",
    "# this gives us values between 0 and 1 for the labels\n",
    "labels = np.linspace(0, 1, 12)\n",
    "\n",
    "def random_label():   \n",
    "    #return labels[np.random.choice([2, 3, 6, 8, 10])] # place here the labels you want to randomly choose from\n",
    "    return np.random.choice(labels[2:]) # simply excludes background and facade\n",
    "    #return np.random.uniform(0.1, 1.0)\n",
    "c = canvas.Canvas(512, 256)\n",
    "c.background(jet_color(0)[:-1]) # Fill with the darkest color (background)\n",
    "c.no_stroke()\n",
    "\n",
    "# Draw the main facade\n",
    "pad = 0\n",
    "c.fill(jet_color(labels[1]))\n",
    "c.rect(pad, pad, 256-pad*2, 256)\n",
    "\n",
    "# Draw some random rectangle with random feature colors\n",
    "for i in range(30):\n",
    "    c.fill(jet_color(random_label()))\n",
    "    c.rect(np.random.uniform(pad, c.height-pad*2, size=2), np.random.uniform(2, 7, size=2)*6)\n",
    "    #c.fill(jet_color(random_label()))\n",
    "    #c.circle(np.random.uniform(pad, c.height-pad, size=2), np.random.uniform(5, c.height*0.15)*0.5) #, size=2))\n",
    "# Get the left half of the canvas image\n",
    "img = c.get_image()[:, :256]\n",
    "\n",
    "# And transform it using our pix2pix model\n",
    "result = generate(model, img.copy())\n",
    "c.image(result, [256, 0])\n",
    "c.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "1c544d3133b9d8c6f36fca025551af31afa9ef134259e7064ad6be0c15e6401c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
