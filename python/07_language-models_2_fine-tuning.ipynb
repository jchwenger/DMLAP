{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSiVr3fyIlGQ"
   },
   "source": [
    "# Language Models 2 | Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSiVr3fyIlGQ"
   },
   "source": [
    "Modified from [Fine-tune a pretrained model](https://huggingface.co/docs/transformers/training).\n",
    "\n",
    "Note, this is (*meant to be used on Colab*(, simply because you will need a fair amount of GPU memory to get these models running! Have a look at [the PyTorch part of `setup.md`](https://github.com/jchwenger/DMLCP/blob/main/setup.md#pytorch--huggingfacegradio) for details on how to make a Huggingface environment on your own machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSiVr3fyIlGQ"
   },
   "source": [
    "## Install & Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSiVr3fyIlGQ",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSiVr3fyIlGQ"
   },
   "source": [
    "If you need to load/save to your drive:\n",
    "\n",
    "```python\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "\n",
    "import os\n",
    "os.chdir('drive/My Drive/IS53055B-DMLCP/DMLCP/python') # to change to another directory\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSiVr3fyIlGQ",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Huggingface login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSiVr3fyIlGQ"
   },
   "source": [
    "For some models and datasets, and if you want to push your model to HF (same as GitHub, but for models) you need to be logged into your HF account.\n",
    "\n",
    "For that, you need to create an account [here](https://huggingface.co/) and then to ['/settings/tokens'](https://huggingface.co/settings/tokens) to create an access token.\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "from huggingface_hub import notebook_login\n",
    "if not (Path.home()/'.huggingface'/'token').exists():\n",
    "    notebook_login()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSiVr3fyIlGQ"
   },
   "source": [
    "#### Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSiVr3fyIlGQ"
   },
   "source": [
    "1. On Colab:\n",
    "\n",
    "```python\n",
    "!pip install -Uq transformers datasets accelerate\n",
    "```\n",
    "\n",
    "2. Locally, the install is the same as the one used for Language models, see [`setup.md`](https://github.com/jchwenger/DMLCP/blob/main/setup.md#pytorch--huggingfacegradio)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "vcMun0XDn3xU",
    "outputId": "cb84f444-4845-478c-e506-618febe53b6e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "# See: https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html#creating-models\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HDzalkV7T0x0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import textwrap # The textwrap module automatically formats text for you\n",
    "\n",
    "tw = textwrap.TextWrapper(   # many more options, see them with textwrap.TextWrapper?\n",
    "    width=79,                # the formatted width we want\n",
    "    replace_whitespace=False # this will keep whitespace & line breaks in the original text\n",
    ")\n",
    "\n",
    "def wrap_print(s):\n",
    "    \"\"\"Format text into Textwrapped lines and print it\"\"\"\n",
    "    print(\"\\n\".join(tw.wrap(s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMcUveahB0pk"
   },
   "source": [
    "## Test the raw gpt-2 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMcUveahB0pk"
   },
   "source": [
    "Hugginface uses [pipelines](https://huggingface.co/docs/transformers/pipeline_tutorial) as the unified high-level API for inference.\n",
    "\n",
    "You can choose from a [large amount of tasks](https://huggingface.co/tasks), and, in each task, from a [huge amount of models](https://huggingface.co/models?sort=trending)! Not only that, but you can also find loads and loads of [datasets](https://huggingface.co/datasets?task_categories=task_categories:text-generation&sort=trending) readily available, and already classified by tasks. In our case, we will be using the [`tiny_shakespeare`](https://huggingface.co/datasets/tiny_shakespeare) dataset, originally created by [Andrej Karpathy](https://karpathy.ai/) to test language models.\n",
    "\n",
    "In this case, you can see [here](https://huggingface.co/models?pipeline_tag=text-generation&sort=trending) the list of models for `text-generation`, and in the search bar if you type `gpt`, the \"classic\" `gpt2` model will appear. This is actually GPT-1, with around 120 millions parameters (GPT-2 has 1.5 billions, and GPT-3 175 billions...).\n",
    "\n",
    "One thing that you can test on Colab directly is to switch between:\n",
    "\n",
    "- `gpt2` (\\~120mio),  \n",
    "- `gpt2-medium` (\\~350mio),  \n",
    "- `gpt2-large` (\\~770mio) and  \n",
    "- `gpt2-xl` (\\~1.5bn) parameters  \n",
    "\n",
    "and see if you notice a difference in the text quality. Without more advanced methods like freezing layers, or quantization + parameter-efficient methods, only the smallest one will fit in a free T4 Colab GPU!\n",
    "\n",
    "**Warning**: these models were trained on large portions of the Internet (more specifically upvoted Reddit articles...). Their output is biased, and often offensive, in the same way as the Internet is!\n",
    "\n",
    "Note: HuggingFace now requires you to define your configuration for generation in advance in an object that you pass to your generator. See [the documentation](https://huggingface.co/docs/transformers/main_classes/text_generation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "npCvBU8BBxVm"
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"gpt2\"\n",
    "\n",
    "generator = pipeline(\n",
    "    'text-generation', # it is quite easy to change task if you want to!\n",
    "    model=MODEL_ID,      # here you can choose your model from: https://huggingface.co/models?pipeline_tag=text-generation&sort=trending\n",
    "    device=device      # device allows you to choose the device to run your model on\n",
    ")\n",
    "\n",
    "# our configuration\n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_ID)\n",
    "generation_config.pad_token_id = generation_config.eos_token_id\n",
    "generation_config.max_length = 250\n",
    "generation_config.do_sample = True\n",
    "generation_config.top_p = 0.95\n",
    "generation_config.temperature = .9\n",
    "generation_config.batch_size = 1\n",
    "\n",
    "# this should *not* sound like Shakespeare...\n",
    "wrap_print(\n",
    "    generator(\n",
    "        \"MERCUTIO:\",\n",
    "        generation_config=generation_config,\n",
    "        num_return_sequences=1\n",
    "    )[0]['generated_text'] # [0] to select the first element (you can generate batches),\n",
    ")                          # this yields an object, and the text lives in 'generated_text'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AjWm-d9H9vta"
   },
   "source": [
    "#### Note on memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AjWm-d9H9vta"
   },
   "source": [
    "If you try several models, before you do any training it is a good idea to clear the memory, using this:\n",
    "```python\n",
    "del generator            # del to delete any Python object\n",
    "torch.cuda.clear_cache() # PyTorch to clear GPU mem\n",
    "                         # (it can take a few moments to be executed!)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8ysxF7_El-n"
   },
   "source": [
    "---\n",
    "\n",
    "##Â 1. Training in Python, with a Trainer class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8ysxF7_El-n"
   },
   "source": [
    "Ported from the [Causal language modeling](https://huggingface.co/docs/transformers/tasks/language_modeling) and [Fine-tune a language model](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb) tutorials. ([*So many other tutorials here, for all sorts of models*](https://huggingface.co/docs/transformers/notebooks)...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Load tokenizer & model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "dRT9Es50BT2c",
    "outputId": "134a14ae-3632-43b1-a0f7-10a6c807b665",
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "# removes some errors with the data collator\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id # use EOS (end of sequence) as padding\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID).to(device) # manually move the model to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ycQ-jOVeURY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "FINETUNED_MODEL_ID = \"gpt2.shak\" # change if you need or work with another dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Load & prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ycQ-jOVeURY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_raw = load_dataset(\"tiny_shakespeare\")\n",
    "\n",
    "print(dataset_raw)\n",
    "print(\"-\" *40)\n",
    "print(\"Number of characters per split:\")\n",
    "print([(split, len(dataset_raw[split][\"text\"][0])) for split in dataset_raw])\n",
    "print(\"-\" *40)\n",
    "print(dataset_raw[\"train\"][\"text\"][0][:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIrPM6XTewJU"
   },
   "source": [
    "See [here](https://huggingface.co/learn/nlp-course/chapter5/3#the-map-methods-superpowers) for an explanation of the `batched=True` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "uSdLl-ipLapj",
    "outputId": "d5b197ac-483d-4910-b91c-d75127aae1f8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])\n",
    "\n",
    "dataset_tok = dataset_raw.map(\n",
    "    tokenize_function,\n",
    "    batched=True,           # this 'batched' says the mapping will happen in parallel\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "# the content of our \"text\" now lives in \"input_ids\", and since we only have one example\n",
    "# with all the text in it, we select it with [0] to see up to 40 tokens\n",
    "print(dataset_tok[\"train\"][\"input_ids\"][0][:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JPKFVXmo2yt8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/transformers/blob/5936c8c57ccb2bda3b3f28856a7ef992c5c9f451/examples/pytorch/language-modeling/run_clm.py#L516\n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n",
    "\n",
    "block_size = tokenizer.max_len_single_sentence # 1024\n",
    "\n",
    "def group_texts(ds):\n",
    "    # Concatenate all texts.\n",
    "    concat_ds = {k: sum(ds[k], []) for k in ds.keys()}\n",
    "    total_length = len(concat_ds[list(ds.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concat_ds.items()\n",
    "    }\n",
    "    # Important note: during training, the two sequences will be shifted by one,\n",
    "    # so that the model predicts the next step for each step. This is done auto-\n",
    "    # matically by Huggingface internally\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_dataset = dataset_tok.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "print(f\"Now our dataset contains {len(lm_dataset['train']['input_ids'])} examples,\")\n",
    "print(f\"each of length {len(lm_dataset['train']['input_ids'][0])} tokens (that we can feed in batches)...\")\n",
    "print(f\"(the 'block_size', aka 'attention window' of our model is {block_size=})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPsOz3uSvCob"
   },
   "source": [
    "### The Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xlpKGOGbBW2P",
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 3 # I'm able to fine-tune the 120mio gpt2 on a 6GB GPU laptop...\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=FINETUNED_MODEL_ID,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,             # small learning rate\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=5,\n",
    "    # max_steps=100,                # overrides num_train_epochs, to train even less than one epoch!\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    # push_to_hub=True,              # uncomment to push to your HF account\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset['train'],\n",
    "    eval_dataset=lm_dataset['validation'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SzYkf6jmUBt1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPk7-VwB_Kpz"
   },
   "source": [
    "Note that in the training above I did 5 epochs, which is overkill! Models tend to suffer from \"catastrophic forgetting\" when you fine-tune, and it's often interesting to train for as little as possible to explore the change in the network output, before training a bit more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPk7-VwB_Kpz",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Note on memory (again)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPk7-VwB_Kpz"
   },
   "source": [
    "When testing things and running into memory issues, as before one (imperfect) way to solve this is to delete the `trainer` variable and clear the GPU cache like so (otherwise, just restart the runtime and re-run only the cells you need):\n",
    "\n",
    "```python\n",
    "del trainer\n",
    "torch.cuda.empty_cache()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOA46EmIs3Z4"
   },
   "source": [
    "### Testing the fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOA46EmIs3Z4"
   },
   "source": [
    "Here I go through the lower level steps of encoding text with our tokenizer, creating a batch of prefixes, defining a config, then generating using the model, instead of using a pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C_u2RreSs3Z5"
   },
   "outputs": [],
   "source": [
    "# encode context the generation is conditioned on\n",
    "input_ids = tokenizer.encode('MERCUTIO:', return_tensors='pt')\n",
    "\n",
    "batched_input_ids = input_ids.repeat(2,1).to(device) # copy and place on GPU\n",
    "\n",
    "# same logic as before\n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_ID)\n",
    "generation_config.pad_token_id = generation_config.eos_token_id\n",
    "generation_config.max_length = 100\n",
    "generation_config.do_sample = True\n",
    "generation_config.top_p = 0.98\n",
    "generation_config.temperature = .9\n",
    "\n",
    "# generate text using the same config as before\n",
    "output = model.generate(\n",
    "    batched_input_ids,\n",
    "    generation_config=generation_config\n",
    ")\n",
    "\n",
    "# decode back from tokens to text\n",
    "texts = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "\n",
    "# print\n",
    "for t in texts:\n",
    "    wrap_print(t)\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VY8QAVEgZmMC"
   },
   "source": [
    "### Saving your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VY8QAVEgZmMC"
   },
   "source": [
    "If you want to save your model manually, you can just do:\n",
    "\n",
    "```python\n",
    "trainer.save_model(FINETUNED_MODEL_ID) # or some other directory\n",
    "```\n",
    "\n",
    "To save the model and tokenizer manually, you can do:\n",
    "\n",
    "```python\n",
    "tokenizer.save_pretrained(FINETUNED_MODEL_ID)\n",
    "model.save_pretrained(FINETUNED_MODEL_ID)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VY8QAVEgZmMC"
   },
   "source": [
    "#### Note on Huggingface Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VY8QAVEgZmMC"
   },
   "source": [
    "[Share a model Huggingface tutorial](https://huggingface.co/docs/transformers/model_sharing)\n",
    "\n",
    " Huggingface `models`, `tokenizers`, and `trainers` all have a `.push_to_hub('my-model')` method, but the `trainer` will be the one saving everything you need.\n",
    "\n",
    " You can push your finetuned pipeline like so:\n",
    "\n",
    " ```python\n",
    " trainer.push_to_hub(FINETUNED_MODEL_ID) # or another name\n",
    " ```\n",
    "\n",
    " Once the model is on the hub, we can create a new pipeline you can now access your model from anywhere using `model='username/your-model-id'` (or any name you used for the output folder). You can also use the folder where you saved your model (`model=/path/to/your/model`).\n",
    "\n",
    " For a full course with videos and notebooks, check the [NLP Course](https://huggingface.co/learn/nlp-course/chapter1/1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hrk0zXsBuT1U"
   },
   "outputs": [],
   "source": [
    "HF_HUB_MODEL_ID = \"jchwenger/gpt2.shak\" # replace with your own\n",
    "\n",
    "generator = pipeline(\n",
    "    'text-generation',\n",
    "    model=HF_HUB_MODEL_ID, # download the model from the hub\n",
    "    tokenizer=tokenizer,         # this time, we need to specify which tokenizer to use\n",
    "    device=device\n",
    ")\n",
    "\n",
    "wrap_print(\n",
    "    generator(\n",
    "        \"MERCUTIO:\",\n",
    "        generation_config=generation_config,\n",
    "        num_return_sequences=1\n",
    "    )[0]['generated_text']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48zRx_M5Cbgy"
   },
   "source": [
    "---\n",
    "\n",
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48zRx_M5Cbgy",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Search for other datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48zRx_M5Cbgy"
   },
   "source": [
    "`tiny_shakespeare` is obviously not the only dataset availabe on the Hub. Another example I came across a few times recently is the [`english_quotes` dataset by Abirate](https://huggingface.co/datasets/Abirate/english_quotes). You can load it like so:\n",
    "\n",
    "```python\n",
    "dataset_raw = load_dataset(\"Abirate/english_quotes\")\n",
    "```\n",
    "\n",
    "The one thing to watch out for is that the text lives in `\"quote\"`, not in `\"text\"`! Your `tokenize_function`, for instance, should then work on `examples[\"quote\"]` rather than `examples[\"text\"]`, and so does the rest of this code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48zRx_M5Cbgy",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Freezing layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48zRx_M5Cbgy"
   },
   "source": [
    "One strategy to save memory and to reduce the impact of fine-tuning is to finetune only *some* layers of the model (usually the top-ones, leaving the base 'frozen'). You can achieve that by:\n",
    "\n",
    "1. Looking at a list of all your layers:\n",
    "\n",
    "```python\n",
    "for i, (name, params) in enumerate(model.named_parameters()):\n",
    "    print(i, name)\n",
    "```\n",
    "2. Setting the `.requires_grad` attribute to `False` for most layers (here I just loop through them and freeze the first 122 layers (see [this link](https://discuss.huggingface.co/t/freeze-lower-layers-with-auto-classification-model/11386/2)):\n",
    "\n",
    "```python\n",
    "layer_threshold = 122\n",
    "for i, (name, params) in enumerate(model.named_parameters()):\n",
    "    if i < layer_threshold:\n",
    "        # print(f\"freezing: {name}\")\n",
    "        params.requires_grad = False\n",
    "```\n",
    "\n",
    "After doing this, you can notice that the memory footprint of the model is much less big on the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48zRx_M5Cbgy",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Working with raw text files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48zRx_M5Cbgy",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "To load data from a file or a directory, see [this reference](https://huggingface.co/docs/datasets/nlp_load).\n",
    "\n",
    "There are various options available, either manually download the file, for instance like so:\n",
    "\n",
    "```python\n",
    "# let's download the tiny shakespeare dataset manually\n",
    "dataset_dir = \"text-dataset\"\n",
    "if not os.path.isdir(dataset_dir):\n",
    "    os.mkdir(dataset_dir)\n",
    "\n",
    "# move into the directory and download the file\n",
    "os.chdir(dataset_dir)\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "os.chdir('..')\n",
    "```\n",
    "\n",
    "Then we can load the dataset from the directory we created:\n",
    "\n",
    "```python\n",
    "dataset_raw_from_dir = load_dataset(\n",
    "    \"text\", # in this case we need \"text\" as a generic name to specify the task\n",
    "    data_dir=dataset_dir,\n",
    "    sample_by=\"document\"  # we indicate we want the whole text\n",
    ")                         # the default is line by line, \"paragraph\" cuts on empty lines\n",
    "# print(dataset_raw_from_dir[\"train\"][\"text\"][0][:250])\n",
    "print(len(dataset_raw_from_dir[\"train\"][\"text\"][0]))\n",
    "```\n",
    "\n",
    "Note that in the code above, `load_dataset` is usable if you have more than one files. You can also select which files go where like so: `data_files={\"train\": \"text-dataset/input.txt\"}`. Even more, you could skip the above step and just do `data_files=='https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'`.\n",
    "\n",
    "When we load raw files, the train/validation/test split is not done for us. *If you wanted to do this* (you could also not care about a validation dataset and just have \"train\", you wouldn't be the first person to do this), for this one file, you would do it this way:\n",
    "\n",
    "```python\n",
    "# adapted from ChatGPT-4 output\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "def split_text_dataset(dataset, train_percent=0.9, validation_percent=0.05):\n",
    "    # Retrieve all texts\n",
    "    full_text = '\\n'.join(dataset[\"train\"][\"text\"])\n",
    "\n",
    "    # Calculate split lengths\n",
    "    total_length = len(full_text)\n",
    "    train_length = int(total_length * train_percent)\n",
    "    validation_length = int(total_length * validation_percent)\n",
    "\n",
    "    # Split the text\n",
    "    train_text = full_text[:train_length]\n",
    "    validation_text = full_text[train_length:train_length + validation_length]\n",
    "    test_text = full_text[train_length + validation_length:]\n",
    "\n",
    "    # Combine into a DatasetDict\n",
    "    return DatasetDict({\n",
    "        'train': Dataset.from_dict({'text': [train_text]}),\n",
    "        'validation': Dataset.from_dict({'text': [validation_text]}),\n",
    "        'test': Dataset.from_dict({'text': [test_text]})\n",
    "    })\n",
    "\n",
    "dataset_raw = split_text_dataset(dataset_raw_from_dir)\n",
    "```\n",
    "\n",
    "Now our dataset is almost the same as the one downloaded from the Hub:\n",
    "\n",
    "```python\n",
    "print(dataset_raw)\n",
    "print([(split, len(dataset_raw[split]['text'][0])) for split in dataset_raw])\n",
    "print(dataset_raw['train']['text'][0][:250])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48zRx_M5Cbgy"
   },
   "source": [
    "### Even deeper: the full GPT pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48zRx_M5Cbgy"
   },
   "source": [
    "If you want to see how deep the rabbit hole goes, I can only recommend [this Colab](https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing#scrollTo=2c5V0FvqseE0) by the same Karpathy, accompanying [his tutorial](https://www.youtube.com/watch?v=kCc8FmEb1nY) (I highly recommend this series on building language models from scratch!).\n",
    "\n",
    "There is also this less low-level HF tutorial [\"Training a causal language model from scratch\n",
    "\"](https://huggingface.co/learn/nlp-course/en/chapter7/6?fw=pt), and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section6_pt.ipynb)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
