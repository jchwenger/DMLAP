{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "foXvSQKubUBS"
   },
   "source": [
    "# Gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "foXvSQKubUBS"
   },
   "source": [
    "Gradio is a web app framework designed to facilitate the development and deployment of ML and DL apps. Have a look at [their website](https://www.gradio.app).\n",
    "\n",
    "The following adapts their [Quickstart Guide](https://www.gradio.app/guides/quickstart)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eMmGDw2IaUkx"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    !pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1AZLCYE8dH8Y"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQjmc5fcclCf"
   },
   "source": [
    "---\n",
    "\n",
    "## 1. Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQjmc5fcclCf"
   },
   "source": [
    "### Hello, World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQjmc5fcclCf"
   },
   "source": [
    "Docs:\n",
    "\n",
    "- [Textbox](https://www.gradio.app/docs/textbox)\n",
    "- [Interface](https://www.gradio.app/docs/interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TkE2Q8gZboTt"
   },
   "outputs": [],
   "source": [
    "def greet(name):\n",
    "    return f\"Hello {name}!\"\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=greet,\n",
    "    inputs=gr.Textbox( # customize your textbox\n",
    "        lines=2,\n",
    "        placeholder=\"Name here...\"\n",
    "        ),\n",
    "    outputs=\"text\"\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXsOk7NDc36M"
   },
   "source": [
    "### Multiple Input and Output Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXsOk7NDc36M"
   },
   "source": [
    "Docs:\n",
    "\n",
    "- [Slider](https://www.gradio.app/docs/slider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "grvYbxeVc3dK"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# how about adding a second \"checkbox\" as a third input\n",
    "# to allow the user to tick whether it's rainy or not,\n",
    "# and add text that changes accordingly?\n",
    "def greet(name, is_morning, temperature):\n",
    "    salutation = \"Good morning\" if is_morning else \"Good evening\"\n",
    "    greeting = f\"{salutation} {name}. It is {temperature} degrees today\"\n",
    "    celsius = (temperature - 32) * 5 / 9\n",
    "    return greeting, round(celsius, 2)\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=greet,\n",
    "    inputs=[\n",
    "        \"text\",\n",
    "        \"checkbox\",\n",
    "        gr.Slider(0, 100) # you can add a default 'value=75' to your slider if you want\n",
    "    ],\n",
    "    outputs=[     # to add labels, try this (thx ChatGPT!):\n",
    "        \"text\",   # gr.Textbox(label=\"Greeting\"),  # Custom label for the first output\n",
    "        \"number\", # gr.Number(label=\"Temperature in Celsius\")  # Custom label for the second output\n",
    "    ],\n",
    ")\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCo66WSzdy2y"
   },
   "source": [
    "### An Image Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCo66WSzdy2y"
   },
   "source": [
    "You are obviously free to do whatever you like with your inputs, and they are not limited to text only! Here is an example where we modify an image.\n",
    "\n",
    "Docs:\n",
    "- [Image](https://www.gradio.app/docs/image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rWBSyWcTd10c"
   },
   "outputs": [],
   "source": [
    "def filter(input_img):\n",
    "    # sepia filter\n",
    "    img_filter = np.array([\n",
    "        [0.393, 0.769, 0.189],\n",
    "        [0.349, 0.686, 0.168],\n",
    "        [0.272, 0.534, 0.131]\n",
    "    ])\n",
    "    img_filter = img_filter.astype(np.float64) # make sure the contents are floats\n",
    "    filter_img = input_img.dot(img_filter.T)\n",
    "    filter_img /= filter_img.max()\n",
    "    return filter_img\n",
    "\n",
    "demo = gr.Interface(filter, gr.Image(), \"image\")\n",
    "\n",
    "demo.launch(show_api=False, debug=True)\n",
    "\n",
    "# There are quite a few matrices that have an interesting effect,\n",
    "# it can be a nice idea to go look for others and modify this app\n",
    "# to allow the user to choose between different filters!\n",
    "# Color Inversion\n",
    "# img_filter = np.array([\n",
    "#     [-1, 0, 0],\n",
    "#     [0, -1, 0],\n",
    "#     [0, 0, -1]\n",
    "# ]) + 1\n",
    "\n",
    "# # Cool Filter\n",
    "# img_filter = np.array([\n",
    "#     [0.9, 0, 0],\n",
    "#     [0, 0.9, 0],\n",
    "#     [0, 0, 1.1]\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UkJEn0NPf4F1"
   },
   "source": [
    "### Using Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UkJEn0NPf4F1"
   },
   "source": [
    "For more control, `Blocks` are the way to go. Here we can play with flipping various kinds of data left/right or upside down!\n",
    "\n",
    "Docs:\n",
    "- [Blocks](https://www.gradio.app/docs/blocks)\n",
    "- [Markdown](https://www.gradio.app/docs/markdown)\n",
    "- [Tab](https://www.gradio.app/docs/tab)\n",
    "- [Button](https://www.gradio.app/docs/button)\n",
    "- [Accordion](https://www.gradio.app/docs/accordion)\n",
    "- [Audio](https://www.gradio.app/docs/audio)\n",
    "\n",
    "Also [`np.fliplr`](https://numpy.org/doc/stable/reference/generated/numpy.fliplr.html) and [`np.flipud`](https://numpy.org/doc/stable/reference/generated/numpy.flipud.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kbqIxuwtf5UD"
   },
   "outputs": [],
   "source": [
    "def flip_text(x):\n",
    "    return x[::-1]\n",
    "\n",
    "def flip_image(x):\n",
    "    return np.fliplr(x)  # try also np.flipud\n",
    "\n",
    "# help from ChatGPT (buggy, but still nice) for this!\n",
    "def reverse_audio(audio_data):\n",
    "    # audio_data is a tuple with (sample_rate, audio_array)\n",
    "    # print(audio_data)\n",
    "    sample_rate, audio_array = audio_data\n",
    "    reversed_audio = audio_array[::-1]  # Reverse the audio data\n",
    "    # Return the reversed audio data along with the sample rate\n",
    "    return (sample_rate, reversed_audio)\n",
    "\n",
    "# note the 'with' syntax, to allow you to populate your blocks\n",
    "with gr.Blocks() as demo:\n",
    "    # use markdown syntax in the app\n",
    "    gr.Markdown(\"# Flip text, image, or audio files using this demo.\")\n",
    "    with gr.Tab(\"Flip Text\"):\n",
    "        text_input = gr.Textbox()\n",
    "        text_output = gr.Textbox()\n",
    "        text_button = gr.Button(\"Flip\")\n",
    "    with gr.Tab(\"Flip Image\"):\n",
    "        with gr.Row():\n",
    "            image_input = gr.Image()\n",
    "            image_output = gr.Image()\n",
    "        image_button = gr.Button(\"Flip\")\n",
    "    with gr.Tab(\"Reverse Audio\"):\n",
    "        with gr.Row():\n",
    "            audio_input = gr.Audio()\n",
    "            audio_output = gr.Audio()\n",
    "        audio_button = gr.Button(\"Reverse\")\n",
    "\n",
    "    with gr.Accordion(\"Open for More!\"):\n",
    "        gr.Markdown(\"Look at me...\")\n",
    "\n",
    "    # now we have three different functions, with three different effects, in each one of our tabs!\n",
    "    text_button.click(flip_text, inputs=text_input, outputs=text_output)\n",
    "    image_button.click(flip_image, inputs=image_input, outputs=image_output)\n",
    "    audio_button.click(reverse_audio, inputs=audio_input, outputs=audio_output)\n",
    "\n",
    "demo.launch(show_api=False, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaSQor7Dgn_I"
   },
   "source": [
    "---\n",
    "\n",
    "## 2. Chatbots!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaSQor7Dgn_I"
   },
   "source": [
    "Adapated from  the [Chatbot](https://www.gradio.app/guides/quickstart#chatbots) part of the Quickstart, [How to Create a Chatbot with Gradio](https://www.gradio.app/guides/creating-a-chatbot-fast#a-streaming-example-using-openai) and [How to Create a Custom Chatbot with Gradio Blocks](https://www.gradio.app/guides/creating-a-custom-chatbot-with-blocks#adding-markdown-images-audio-or-videos).\n",
    "\n",
    "Gradio apps these days are used for two main purposes:\n",
    "- to show off diffusion models (we are doing this too!);\n",
    "- to build chatbots.\n",
    "\n",
    "The [triple threat](https://en.wiktionary.org/wiki/triple_threat) of Huggingface and its Hub of models/tokenizers/datasets, Gradio apps<sup>1</sup> and [Huggingface Spaces](https://huggingface.co/spaces) (to deploy apps and provide GPUS) had made this open-source corp hugely important in the field, and turbocharged the development of this ecosystem.\n",
    "\n",
    "Let's look at some chatbot examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaSQor7Dgn_I"
   },
   "source": [
    "#### Check out [Huggingface Spaces](https://huggingface.co/spaces)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaSQor7Dgn_I"
   },
   "source": [
    "That works like GitHub in that you create repository following a certain syntax for your app, and then you can edit the app locally, push it to Spaces and see the updates (even if you don't have a GPU on your machine). And just like on GitHub, you can *fork* projects (copy and import someone else's code into your own account and then modify it to make it your own). Check out the [intro guide](https://huggingface.co/spaces/launch).\n",
    "\n",
    "\n",
    "<small>1: note that there's also [Streamlit](https://streamlit.io/), let me know if you tried that and found it better! And it seems now [Docker](https://www.docker.com/) might also be supported on Spaces 🥳</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_mU5gMtgzMD"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    !pip -q install gradio\n",
    "    !pip -q install transformers\n",
    "\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HvkKNor_hCZg"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWJIXorgl08Z"
   },
   "source": [
    "### Example: a chatbot that responds yes or no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWJIXorgl08Z"
   },
   "source": [
    "This is obviously the simplest possible thing you can do, but this logic of randomness can be combined with branches (`if/else`) to create all sorts of elaborate paths of decision guiding your bot's answers: attempts at chatbots before Deep Learning would use elaborate systems like this one (for the literature lovers, see [this](https://drive.google.com/file/d/1v-q2M8ZlCcoCGKvw0rgpmyBg6bbH4DWF/view?usp=sharing), [that](https://drive.google.com/file/d/12-wIbonK8d8w5UXpFM_-nzMfBoDpq4Zq/view?usp=sharing) and [that](https://drive.google.com/file/d/1G_T2MjCQCuLYIQPpe0GnBOu3PO67deyL/view?usp=sharing) articles, for instance).\n",
    "\n",
    "Docs:\n",
    "- [ChatInterface](https://www.gradio.app/docs/chatinterface)\n",
    "\n",
    "And the [`random.choice`](https://docs.python.org/3/library/random.html#functions-for-sequences) function in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s0m69KfWlvDG"
   },
   "outputs": [],
   "source": [
    "def random_response(message, history):\n",
    "    return random.choice([\"Yes\", \"No\"])\n",
    "\n",
    "gr.ChatInterface(random_response).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WOzUno_mEnq"
   },
   "source": [
    "### Another example using the user’s input and history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WOzUno_mEnq"
   },
   "source": [
    "Here you can see that I use both a modulo logic (a bit boring, randomness might be nicer) to decide between three different kinds of answer. Then, in option 2, I randomly pick one interaction in the history, and use the user input!\n",
    "\n",
    "Reminder: `history` is mentioned [here](https://www.gradio.app/guides/quickstart#chatbots)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JSAucYRumLZ5"
   },
   "outputs": [],
   "source": [
    "def sometimes_agree_sometimes_remembers(message, history):\n",
    "    # print(*history, sep=\"\\n\") # you can have a look at the history object if you want\n",
    "    if len(history) % 3 == 0:\n",
    "        return f\"Yes, I do think that '{message}'\"\n",
    "    elif len(history) % 3 == 1:\n",
    "        # history is an array of arrays, each containing [\"user input\", \"bot response\"]\n",
    "        past_message = random.choice(history)[0] # [0] for user input, [1] for bot response\n",
    "        return f\"Wait, didn't you say earlier: '{past_message}'\"\n",
    "    else:\n",
    "        return \"I don't think so\"\n",
    "\n",
    "gr.ChatInterface(sometimes_agree_sometimes_remembers).launch(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "laxm3HaAmY_O"
   },
   "source": [
    "### Streaming chatbots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "laxm3HaAmY_O"
   },
   "source": [
    "Isn't it nice if your bot types live instead of giving you a full answer?\n",
    "\n",
    "For that, and any gradual process of answering, use the [`yield` keyword of Python generators](https://realpython.com/introduction-to-python-generators/).\n",
    "\n",
    "See [this part](https://www.gradio.app/guides/creating-a-chatbot-fast#a-streaming-example-using-openai)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tRQinLJgmZtR"
   },
   "outputs": [],
   "source": [
    "def slow_echo(message, history):\n",
    "    for i in range(len(message)):\n",
    "        time.sleep(0.3) # try random.random() * .5 (or another number) for irregular typing speed!\n",
    "        yield f\"You typed: {message[: i+1]}\"\n",
    "\n",
    "gr.ChatInterface(slow_echo).queue().launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-hakvnRmwj4"
   },
   "source": [
    "### A customised Yes-Bot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-hakvnRmwj4"
   },
   "source": [
    "See [this part](https://www.gradio.app/guides/creating-a-chatbot-fast#customizing-your-chatbot).\n",
    "\n",
    "Docs & more:\n",
    "- [Descriptive content](https://www.gradio.app/guides/key-features#descriptive-content)\n",
    "- [Examples](https://www.gradio.app/guides/more-on-examples)\n",
    "- [Styling](https://www.gradio.app/guides/key-features#styling)\n",
    "- [Theming guide](https://www.gradio.app/guides/theming-guide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yIC8oERvmyMy"
   },
   "outputs": [],
   "source": [
    "def yes_bot(message, history):\n",
    "    if message.endswith(\"?\"):\n",
    "        # silly fun with yes answers\n",
    "        return random.choice(\n",
    "            [\"Yes\", \"Yes yes yes!\", \"Whoa, so totally yes!\", \"Hell yes!\", \"Man, I couldn't agree more!\", \"Absolutely!\", \"Haha I thought this too!\"]\n",
    "            )\n",
    "    else:\n",
    "        return \"Ask me anything!\"\n",
    "\n",
    "gr.ChatInterface(\n",
    "    yes_bot,\n",
    "    chatbot=gr.Chatbot(height=300),\n",
    "    textbox=gr.Textbox(placeholder=\"Ask me a yes or no question\", container=False, scale=7),\n",
    "    title=\"Yes Bot\",\n",
    "    description=\"Ask Yes Man any question\",\n",
    "    theme=\"soft\",\n",
    "    examples=[\"Hello\", \"Am I cool?\", \"Are tomatoes vegetables?\"],\n",
    "    cache_examples=True,\n",
    "    retry_btn=None,\n",
    "    undo_btn=\"Delete Previous\",\n",
    "    clear_btn=\"Clear\",\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGgN5c9VnaOT"
   },
   "source": [
    "### Additional Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGgN5c9VnaOT"
   },
   "source": [
    "See [this part](https://www.gradio.app/guides/creating-a-chatbot-fast#additional-inputs).\n",
    "\n",
    "You may want to add additional parameters to your chatbot and expose them to your users through the Chatbot UI. For example, suppose you want to add a textbox for a system prompt, or a slider that sets the number of tokens in the chatbot's response. The `ChatInterface` class supports an `additional_inputs` parameter which can be used to add additional input components.\n",
    "\n",
    "The `additional_inputs` parameters accepts a component or a list of components. You can pass the component instances directly, or use their string shortcuts (e.g. \"`textbox`\" instead of `gr.Textbox()`). If you pass in component instances, and they have not already been rendered, then the components will appear underneath the chatbot (and any examples) within a `gr.Accordion()`. You can set the label of this accordion using the `additional_inputs_accordion_name parameter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pVNTT7uUncpz"
   },
   "outputs": [],
   "source": [
    "def echo(message, history, system_prompt, max_tokens):\n",
    "    # This is very useful, as it allows to modify the behaviour of the\n",
    "    # chatbot on top of the user input (the system prompt, in this case,\n",
    "    # is re-added every single time, without the user having to re-type it)\n",
    "    response = f\"System prompt: {system_prompt}\\n Message: {message}.\"\n",
    "    # of course, in a real chatbot like below, you would feed the message\n",
    "    # to the chatbot, then stream the completion by the neural net!\n",
    "    for i in range(min(len(response), int(max_tokens))):\n",
    "        time.sleep(0.05) # again, you could use a random logic for a better typing effect\n",
    "        yield response[: i+1]\n",
    "\n",
    "demo = gr.ChatInterface(\n",
    "    echo,\n",
    "    additional_inputs=[ # this is displayed in an accordion below the box\n",
    "        gr.Textbox(\"You are helpful AI.\", label=\"System Prompt\"),\n",
    "        gr.Slider(10, 100, label=\"Max tokens\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "demo.queue().launch(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k892p0HVh7cZ"
   },
   "source": [
    "### Example using a local, open-source LLM with Hugging Face\n",
    "\n",
    "See [this part](https://www.gradio.app/guides/creating-a-chatbot-fast#example-using-a-local-open-source-llm-with-hugging-face).\n",
    "\n",
    "The model we use is [RedPajama](https://huggingface.co/togethercomputer/RedPajama-INCITE-Chat-3B-v1) (pretty big, best run on Colab!), see also [this post for the dataset](https://www.together.ai/blog/redpajama-data-v2). Chat models are regular language models finetuned on specific chat datasets (especially, they include markers for \"user input\" and \"assistant responses\", as well as, sometimes, overall directives like \"system prompt\" (defining the overall identity of the bot). In Huggingface, you would recognise them as having a \"-chat\" identifier, for instance for the [Llama 2 family](https://huggingface.co/meta-llama).\n",
    "\n",
    "Docs:\n",
    "- [StopingCriteria](https://huggingface.co/docs/transformers/internal/generation_utils#transformers.StoppingCriteria) \\(see also [this nice post](https://discuss.huggingface.co/t/implimentation-of-stopping-criteria-list/20040/2)\\)\n",
    "- [TextIteratorStreamer](https://huggingface.co/docs/transformers/internal/generation_utils#transformers.TextIteratorStreamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KRhQNjVTpSMC"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "# See: https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html#creating-models\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import StoppingCriteria\n",
    "from transformers import StoppingCriteriaList\n",
    "from transformers import TextIteratorStreamer\n",
    "\n",
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTuRsLf7QEqG"
   },
   "source": [
    "**Note**\n",
    "\n",
    "This chatbot uses almost all the memory of a free Colab instance. Unfortunately, I haven't been able to free the memory so that I would be able to restart this app for debugging without restarting the runtime (and re-downloading the model) 😬.\n",
    "\n",
    "The upside is: it is quite powerful! Try speak to it in different languages, or ask it code questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4drDf7PRg2n_"
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"togethercomputer/RedPajama-INCITE-Chat-3B-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype=torch.float16)\n",
    "model = model.to(device) # move model to GPU\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    \"\"\"\n",
    "    Class used `stopping_criteria` in `generate_kwargs` that provides an additional\n",
    "    way of stopping the generation loop (if this class returns `True` on a token,\n",
    "    the generation is stopped)).\n",
    "    \"\"\"\n",
    "    # note: Python now supports type hints, see this: https://realpython.com/lessons/type-hinting/\n",
    "    #       (for the **kwargs see also: https://realpython.com/python-kwargs-and-args/)\n",
    "    # this could also be written: def __call__(self, input_ids, scores, **kwargs):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        stop_ids = [29, 0] # see the cell below to understand where these come from\n",
    "        for stop_id in stop_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "def predict(message, history):\n",
    "\n",
    "    history_transformer_format = history + [[message, \"\"]]\n",
    "    stop = StopOnTokens()\n",
    "\n",
    "    # useful to debug\n",
    "    # msg = \"history\"\n",
    "    # print(msg)\n",
    "    # print(*history_transformer_format, sep=\"\\n\")\n",
    "    # print(\"***\")\n",
    "\n",
    "    # at each step, we feed the entire history in string format,\n",
    "    # restoring the format used in their dataset with new lines\n",
    "    # and <human>: or <bot>: added before the messages\n",
    "    messages = \"\".join(\n",
    "        [\"\".join(\n",
    "            [\"\\n<human>:\"+item[0], \"\\n<bot>:\"+item[1]]\n",
    "         )\n",
    "        for item in history_transformer_format]\n",
    "    )\n",
    "    # # to see what we feed to our net:\n",
    "    # msg = \"string prompt\"\n",
    "    # print(msg)\n",
    "    # print(\"-\" * len(msg))\n",
    "    # print(messages)\n",
    "    # print(\"-\" * 40)\n",
    "\n",
    "    # convert the string into tensors & move to GPU\n",
    "    model_inputs = tokenizer([messages], return_tensors=\"pt\").to(device)\n",
    "\n",
    "    streamer = TextIteratorStreamer(\n",
    "        tokenizer,\n",
    "        # timeout=30.,    # without the timeout, if there's an issue the bot will hang indefinitely\n",
    "        skip_prompt=True, # (haven't implemented the error handling yet 🙈)\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    generate_kwargs = dict(\n",
    "        model_inputs,\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        top_k=1000,\n",
    "        temperature=1.0,\n",
    "        pad_token_id=tokenizer.eos_token_id, # mute annoying warning: https://stackoverflow.com/a/71397707\n",
    "        num_beams=1,  # this is for beam search (disabled), see: https://huggingface.co/blog/how-to-generate#beam-search\n",
    "        stopping_criteria=StoppingCriteriaList([stop])\n",
    "    )\n",
    "    t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
    "    t.start()\n",
    "\n",
    "    partial_message  = \"\"\n",
    "    for new_token in streamer:\n",
    "        # seen the format <human>: and \\n<bot> above (when 'messages' is defined)?\n",
    "        # we stream the message *until* we encounter '<', which is by the end\n",
    "        if new_token != '<':\n",
    "            partial_message += new_token\n",
    "            yield partial_message\n",
    "\n",
    "\n",
    "gr.ChatInterface(predict).queue().launch(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMXQCQzZJImR"
   },
   "source": [
    "How do we know what the stop words are? (This is in part a design choice!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hya1CiBNFbwQ"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "print(\"The model stop words are:\")\n",
    "for tok in [29, 0]:\n",
    "    print(f\"  - `{tokenizer.decode([tok])}`\")\n",
    "\n",
    "print(\"If you wanted to know what token was associated with `<`, you'd do the opposite:\")\n",
    "print(\"`<` encoded as:\", tokenizer.encode(\"<\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZU6FJn1jF4g"
   },
   "source": [
    "---\n",
    "\n",
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZU6FJn1jF4g"
   },
   "source": [
    "- You could try to modify this code to work with the latest Llama models by Meta (you must register on [their site](https://ai.meta.com/llama/), then on Huggingface once you get permission, to be able to download the code). After that (same as with various restricted models/datasets/etc. on the Hub), you would need to log into HF:\n",
    "```python\n",
    "from pathlib import Path\n",
    "from huggingface_hub import notebook_login\n",
    "if not (Path.home()/'.huggingface'/'token').exists():\n",
    "    notebook_login()\n",
    "```\n",
    "- Another example that would allow you to play with the cutting-edge LLMs is the [OpenAI example](https://www.gradio.app/guides/creating-a-chatbot-fast#a-streaming-example-using-openai) in the Gradio tutorial. You would first need to register (with credit card) and get an API key on [their website](https://platform.openai.com/)...\n",
    "\n",
    "- Gradio ships with a [`Flagging`](https://www.gradio.app/guides/key-features#styling) logic, that allows you to harvest data from your users for free! You can also implement [`likes`](https://www.gradio.app/guides/creating-a-custom-chatbot-with-blocks#liking-disliking-chat-messages), that could be interesting!\n",
    "\n",
    "- The current trend these days is to work with multimodality (systems that are able to handle more than one type of data: text and images, for instance, or text and music). See [this last part](https://www.gradio.app/guides/creating-a-custom-chatbot-with-blocks#adding-markdown-images-audio-or-videos) of the Gradio Chatbot tutorial for examples, as well as the two apps they recommend [project-baize/Baize-7B](https://huggingface.co/spaces/project-baize/chat-with-baize) and [MAGAer13/mPLUG-Owl](https://huggingface.co/spaces/MAGAer13/mPLUG-Owl) (and as said you could clone these projects, study the code, and transform them into your own project)!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
