{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello World: Training a Simple Model on the MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to get familiar with using a deep learning library like Tensorflow to train a simple neural network. The network will be trained on the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) which contains small images of handwritten numerical digits. By the end of this training, the model should be able to accurately classify images with numerical digits.\n",
    "\n",
    "Training a network on the MNIST dataset has become the 'hello world' of machine learning. \n",
    "\n",
    "The following is based on the [PyTorch Quickstart](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html), have a look on these pages for more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter (Locally)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recommended way is to clone the whole repo. You will need `pytorch`, `numpy` and `matplotlib` installed. The same commands can be used as for Google Colab below, except in a terminal pointing to the repository, and without the leading `!`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Colab: Two Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Clone the repo inside your Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this, you need to mount your drive to the machine, like so:\n",
    "\n",
    "```python\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# change directory using the os module\n",
    "import os\n",
    "os.chdir('drive/My Drive/')\n",
    "os.listdir()             # shows the contents of the current dir, you can use chdir again after that\n",
    "# os.mkdir(\"IS53055B-DMLCP\") # creating a directory\n",
    "# os.chdir(\"IS53055B-DMLCP\") # moving to this directory\n",
    "# os.getcwd()            # printing the current directory\n",
    "```\n",
    "\n",
    "You can use git in Colab:\n",
    "```python\n",
    "!git clone https://github.com/jchwenger/DMLCP\n",
    "```\n",
    "\n",
    "To pull updates from the upstream repository without losing your work:\n",
    "```python\n",
    "!git stash     # temporary stashing away your changes\n",
    "!git pull      # importing the update from github\n",
    "!git stash pop # reimporting your changes, deleting the stash\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Using this notebook as a standalone file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On Google Colab you will need to download things:\n",
    "\n",
    "```python\n",
    "!curl -O https://raw.githubusercontent.com/jchwenger/DMLCP/main/python/images/3.png\n",
    "!curl -O https://raw.githubusercontent.com/jchwenger/DMLCP/main/python/images/4.png\n",
    "# for FashionMNIST\n",
    "!curl -O https://raw.githubusercontent.com/jchwenger/DMLCP/main/python/images/handbag.png\n",
    "!mkdir images\n",
    "!mv 3.png 4.png handbag.png images\n",
    "!unzip images.zip\n",
    "```\n",
    "\n",
    "But to use the model created by this notebook in another notebook, you will need to either manually download/upload the model file (top left bar has a file explorer), or setup your notebook to mount (= connect to) a Google drive (using the code above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorials & references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python:\n",
    "- [Working With Files](https://realpython.com/working-with-files-in-python/)\n",
    "- [Python's pathlib Module: Taming the File System](https://realpython.com/python-pathlib/)\n",
    "\n",
    "Colab:\n",
    "- [Loading/Saving data on Google Colab](https://colab.research.google.com/notebooks/io.ipynb)\n",
    "\n",
    "Pytorch:\n",
    "- [Transforming and augmenting images](https://pytorch.org/vision/main/transforms.html)\n",
    "- [Getting started with transforms v2](https://pytorch.org/vision/main/auto_examples/transforms/plot_transforms_getting_started.html#sphx-glr-auto-examples-transforms-plot-transforms-getting-started-py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision as tv\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "# Get cpu, gpu or mps device for training\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data processing: walkthrough\n",
    "\n",
    "MNIST contains images of single digits, so 10 classes, from 0 to 9.\n",
    "\n",
    "All images are 28 by 28 pixels, black and white (1 channel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Model / data parameters\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "INPUT_SHAPE = (1,28,28)\n",
    "\n",
    "# fixed directory structure -------------\n",
    "DATASETS_DIR = pathlib.Path(\"datasets\")\n",
    "DATASETS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "MODELS_DIR = pathlib.Path(\"models\")\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "# ----------------------------------------\n",
    "\n",
    "MODEL_NAME = \"dense_mnist\" # change accordingly\n",
    "\n",
    "MNIST_DIR = MODELS_DIR / MODEL_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [here](https://pytorch.org/vision/stable/transforms.html#performance-considerations) for `ToImage` and `ToDtype` to converts a PIL image or NumPy `ndarray` into a `FloatTensor`. and scales the image’s pixel intensity values in the range .[0., 1.]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True)\n",
    "])\n",
    "\n",
    "train_data = tv.datasets.MNIST(\n",
    "    root=DATASETS_DIR,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms,\n",
    ")\n",
    "\n",
    "test_data = tv.datasets.MNIST(\n",
    "    root=DATASETS_DIR,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Why do we split our data like this? That is because we want to see how well our model performs on data *it was not trained on* (the test set)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### A look at our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data)\n",
    "print()\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join(train_data.classes)) # join an array into a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(train_data.data.shape, train_data.data.dtype)\n",
    "print(train_data.targets.shape, train_data.targets.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the difference between the original data type and range, and what happens when you 'call' the dataset to extract a batch, as the model will do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original data type:    {train_data.data.dtype}\")\n",
    "print(f\"range:                 [{train_data.data.min().item()}: {train_data.data.max().item()}]\")\n",
    "print(f\"Transformed data type: {train_data[0][0].dtype}\")\n",
    "print(f\"range:                 [{train_data[0][0].min().item()}: {train_data[0][0].max().item()}]\")\n",
    "print()\n",
    "\n",
    "print(f\"Original label shape:    {train_data.targets.shape} (60k integers)\")\n",
    "print(f\"dtype:                   {train_data.targets.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.set_printoptions(linewidth=150) # prevent wrapping\n",
    "print(f\"This should be a {train_data.classes[test_data.targets[0]]}...\")\n",
    "print()\n",
    "print(test_data.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We can also display the array as an image with matplotlib!\n",
    "def display_sample(dataset, index, pred=None):\n",
    "    plt.figure()\n",
    "    if pred is not None:\n",
    "        plt.title(f\"Label: {dataset.classes[dataset.targets[index]]}, Prediction: {dataset.classes[pred]} | {'CORRECT' if dataset.targets[index] == pred else 'WRONG'}\")\n",
    "    else:\n",
    "        plt.title(f\"Label: {dataset.classes[dataset.targets[index]]}\")\n",
    "    plt.imshow(dataset.data[index], cmap='gray')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "display_sample(test_data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Note: One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, the implementation of the loss we are using, the cross-entropy, accepts integer labels, whereas in various cases (and in the default version of Keras), that same loss accepts one-hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_transform = tv.transforms.Lambda(\n",
    "    lambda y: torch.zeros(NUM_CLASSES, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1)\n",
    ")\n",
    "x = 3\n",
    "print(x)\n",
    "print(target_transform(x)) # one-hot representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "1. **Model definition**: what kind of model do we want? Create a blueprint.\n",
    "2. **Loss & Optimizer**: tell TF to build the model for us.\n",
    "3. **Define our Training loop**    \n",
    "   Also: _Test before training_ (optional): how lousy are we before we start?\n",
    "4. **Training**: aka 'fitting' the model to the data\n",
    "5. **Testing**: how good are we now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition\n",
    "\n",
    "[Model layers](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html#nn-relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten() # [1, 28, 28] -> [1, 28*28]\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(INPUT_SHAPE[1] * INPUT_SHAPE[2], 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A look at our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model structure:\")\n",
    "print(model)\n",
    "print()\n",
    "\n",
    "print(\"Layers:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\" - {name} | Shape: {param.shape}\")\n",
    "\n",
    "print()\n",
    "print(f\"Our model has {sum(p.numel() for p in model.parameters())} parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loss & Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **loss** is how we measure how good our performance is. `CrossEntropy` means:\n",
    "- **crossentropy**: in probability, the cross-entropy loss is a measure of how two probability distributions differ. It calculates the 'distance' between our predictions (a probability distribution) and our labels (*also* a probability distribution, with a 1 where the ground truth is, and zero everywhere else).\n",
    "\n",
    "The **optimizer** will take this loss, and change the parameters of the network in order to improve its preformance. The [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam) optimizer usually works well out of the box (although it requires a fair amount of memory). You can try different [optimizers](https://pytorch.org/docs/stable/optim.html) from the PyTorch API.\n",
    "\n",
    "[nn.CrossEntropyLoss()](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html#loss-function) does not expect one-hot vector labels (as e.g. in Keras/Tensorflow), only integers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9) # you can also add momentum=0.9\n",
    "\n",
    "# other optimizers are available\n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=1e-3)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define our training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    losses, accs = [], []\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        \n",
    "        # 0: data & target to device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 1: prediction\n",
    "        pred = model(X)\n",
    "\n",
    "        # 2: loss\n",
    "        loss = loss_fn(pred, y)\n",
    "    \n",
    "        # 3: 'backward' | Backpropagation!\n",
    "        loss.backward()\n",
    "\n",
    "        # 4: 'step'\n",
    "        optimizer.step()\n",
    "\n",
    "        # 5: 'zero grad' (otherwise the gradients remain there)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Logging & saving history\n",
    "\n",
    "        # save losses\n",
    "        losses.append(loss.item())\n",
    "        # save our accuracy\n",
    "        accs.append((pred.argmax(1) == y).type(torch.float).mean().item())\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    print()\n",
    "    return losses, accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    \n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    losses, accs = [], []\n",
    "    \n",
    "    # no gradients: we are not training!\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            # to device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            # prediction\n",
    "            pred = model(X)\n",
    "            # accumulate loss\n",
    "            t_l = loss_fn(pred, y).item()\n",
    "            test_loss += t_l\n",
    "\n",
    "            # accumulate our accuracy\n",
    "            a = (pred.argmax(1) == y).type(torch.float)\n",
    "            correct += a.sum().item()\n",
    "\n",
    "            # save loss and acc\n",
    "            losses.append(t_l)\n",
    "            accs.append(a.mean().item())\n",
    "    \n",
    "    # average loss & results\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    \n",
    "    print(\"Test Error:\")\n",
    "    print(f\"Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\")\n",
    "    print()\n",
    "    \n",
    "    return losses, accs, correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before training: how good (bad) is our untrained model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _ = test(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "n = torch.randint(0, len(test_data), (1,)).item()\n",
    "x, y = test_data[n][0], test_data[n][1] \n",
    "with torch.no_grad():\n",
    "    x = x.to(device)\n",
    "    pred = model(x)\n",
    "    predicted, actual = pred[0].argmax(0), y\n",
    "    display_sample(test_data, n, pred=predicted.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two parameters we need to define, the `batch_size` and the number of `epochs`.\n",
    "\n",
    "The number of `epochs` defines how many iterations we perform over the dataset over training. The more epochs in training we perform, the longer training is going to take, but it often (but not always) leads to better performance.\n",
    "\n",
    "The `batch_size` defines how many data samples we process in parallel during training, this helps speed up training if we use a bigger batch size (but is dependent on the size of the memory of our computer). Using a higher batch size generally leads to better results training, as the weights are updated based on the loss of the whole batch, which leads to more stable training than if we were to update the weights after each single example. Training in batches is a form of *regularisation* – something that will come up again and again with different tricks for getting the best performance out of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "train_losses, train_accs, test_losses, test_accs = [], [], [], []\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\")\n",
    "    print(\"-------------------------------\")\n",
    "    train_l, train_a = train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_l, test_a, _ = test(test_dataloader, model, loss_fn)\n",
    "    # save history\n",
    "    train_losses.extend(train_l)\n",
    "    train_accs.extend(train_a)\n",
    "    test_losses.extend(test_l)\n",
    "    test_accs.extend(test_a)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. After training: evaluating again (for real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = test(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using our model (with an actual input image)\n",
    "\n",
    "\n",
    "See [Compose](https://pytorch.org/vision/main/generated/torchvision.transforms.v2.Compose.html) and [Transforming and Augmenting Images](https://pytorch.org/vision/stable/transforms.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('images/3.png') # try also images/4.png\n",
    "\n",
    "transforms = v2.Compose([  \n",
    "    tv.transforms.Grayscale(num_output_channels=1),\n",
    "    tv.transforms.Resize(size=(28,28), antialias=True),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True), # from [0,255] to [0,1]\n",
    "])\n",
    "\n",
    "input = transforms(img)\n",
    "input = input.to(device)\n",
    "\n",
    "print(f\"Input shape: {input.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = nn.Softmax(dim=-1)(model(input)).cpu().numpy()\n",
    "print(f\"Our predictions (shape: {predictions.shape})\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot our predictions using a [bar chart](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.bar.html) (sometimes the net is so confident that you will really see just one bar, the other numbers being so small!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.title(\"Predictions\")\n",
    "xs = train_data.classes   # 0 to 9 for xs, our ys are our predictions\n",
    "plt.bar(xs, predictions[0])             # a bar chart\n",
    "plt.xticks(xs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# note that predictions is still *batched* (shape: (1,10)), we need to fetch the first array\n",
    "predicted = np.argmax(predictions[0]) # argmax: the *index* of the highest prediction\n",
    "\n",
    "plt.figure()\n",
    "plt.title(f'Predicted number: {train_data.classes[predicted]}') # use the predicted category in the title\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving & Loading Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# save (reload using torch.jit.load)\n",
    "torch.jit.save(torch.jit.script(model), MNIST_DIR / f\"{MODEL_NAME}_scripted.pt\")\n",
    "\n",
    "# save (reload using model.load_state_dict, requires the model class!)\n",
    "torch.save(model.state_dict(), MNIST_DIR / f\"{MODEL_NAME}.pt\")\n",
    "print(f\"Saved PyTorch Model State to {MNIST_DIR / MODEL_NAME}.pt\")\n",
    "\n",
    "# instantiate then load (you need to have defined NeuralNetwork)!\n",
    "model_reloaded = NeuralNetwork().to(device)\n",
    "model_reloaded.load_state_dict(torch.load(MNIST_DIR / f\"{MODEL_NAME}.pt\", weights_only=True))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Next steps\n",
    "\n",
    "First of all, try and test your model with your own images of numbers (or pulled from the web)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Plotting the evolution of your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a function that allows you to plot data from your history object.\n",
    "\n",
    "```python\n",
    "def plot_training(train_losses, train_accuracies, test_losses=None, test_accuracies=None):\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(10,6))\n",
    "    \n",
    "    # loss\n",
    "    axes[0, 0].set_title(\"Training Loss\")\n",
    "    axes[0, 0].plot(train_losses, label=\"loss\", c=\"c\")\n",
    "    axes[0, 0].legend()  \n",
    "\n",
    "    # accuracy\n",
    "    axes[1, 0].set_title(\"Train Accuracy\")\n",
    "    axes[1, 0].plot(train_accuracies, label=\"accuracy\", c=\"m\")\n",
    "    axes[1, 0].legend()     \n",
    "\n",
    "    if test_losses is not None:\n",
    "        axes[0, 1].set_title(\"Test Loss\")\n",
    "        axes[0, 1].plot(test_losses, label=\"loss\", c=\"c\")\n",
    "        axes[0, 1].legend()\n",
    "    else:\n",
    "        axes[0, 1].axes(\"off\")\n",
    "\n",
    "    if test_accuracies is not None:\n",
    "        axes[1, 1].set_title(\"Test Accuracy\")\n",
    "        axes[1, 1].plot(test_accuracies, label=\"accuracy\", c=\"m\")\n",
    "        axes[1, 1].legend()\n",
    "    else:\n",
    "        axes[1, 1].axes(\"off\")    \n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_training(train_losses, train_accs, test_losses, test_accs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Test on another dataset: FashionMNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try **Fashion MNIST** instead, which works exactly the same way, but with items of clothing instead of numbers! (Can you modify the `matplotlib` code to display the correct class name?)\n",
    "\n",
    "```python\n",
    "train_data = tv.datasets.FashionMNIST(\n",
    "    root=DATASETS_DIR,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms\n",
    ")\n",
    "test_data = tv.datasets.FashionMNIST(\n",
    "    root=DATASETS_DIR,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms\n",
    ")\n",
    "# have a look at the classes:\n",
    "print(train_data.classes)\n",
    "```\n",
    "\n",
    "Many more datasets [here](https://pytorch.org/vision/main/datasets.html#image-classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Note: validation and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technically, if you wanted to optimise your net, you would not use the `test_data` in the training loop, and instead split `train_data` once more, like so:\n",
    "\n",
    "```python\n",
    "partial_train_data, validation_data = torch.utils.data.random_split(train_data, [.9,.1])\n",
    "print(len(partial_train_data), len(validation_data))\n",
    "```\n",
    "\n",
    "Then you would create three `DataLoader`s, and reserve the test data only for testing your model at the very end (a bit like what we did with the saved picture).\n",
    "\n",
    "```python\n",
    "partial_train_dataloader = torch.utils.data.DataLoader(partial_train_data, batch_size=BATCH_SIZE)\n",
    "validation_dataloader = torch.utils.data.DataLoader(validation_data, batch_size=BATCH_SIZE)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Extra: A Mini ConvNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those who feel like exploring the Deep, here's how you would go about replacing the fully connected network above by a small ConvNet:\n",
    "\n",
    "A Convnet will not need images to be flattened, but it will need a channel dimension.\n",
    "\n",
    "1. Change your model definition:\n",
    "\n",
    "```python\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        # 1 input channel, 32 output channels, 3x3 kernel, (default: stride 1, padding 0)\n",
    "        self.conv1 = nn.Conv2d(INPUT_SHAPE[0], 32, 3)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        # this is not automatic: either you build your net\n",
    "        # gradually and print the shapes, or you use the conv & maxpool formulas...\n",
    "        self.flat_dim = 64 * 5 * 5 # 64 filters, channels of 5x5\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(self.flat_dim, NUM_CLASSES)\n",
    "\n",
    "    def forward(self, x, verbose=False):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        if verbose: print(x.size())\n",
    "        x = self.pool1(x)\n",
    "        if verbose: print(x.size())\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        if verbose: print(x.size())\n",
    "        x = self.pool2(x)\n",
    "        if verbose: print(x.size())\n",
    "        \n",
    "        x = x.view(-1, self.flat_dim) # this works for (1,28,28) or (1,1,28,28) \n",
    "        if verbose: print(x.size())\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        if verbose: print(x.size())\n",
    "        return x\n",
    "\n",
    "model = ConvNet().to(device)\n",
    "\n",
    "# passing random data through our net allows us to print intermediate sizes\n",
    "x = model(torch.randn((*INPUT_SHAPE)).to(device), verbose=True)\n",
    "```\n",
    "\n",
    "Voilà! And lastly:\n",
    "\n",
    "2. The `ConvNet` is designed so that you can pass either one image `(1,28,28)` or a batch `(1,1,28,28)` without a problem.\n",
    "\n",
    "3. Save with a different name:\n",
    "\n",
    "With JIT:\n",
    "```python\n",
    "MODEL_NAME = \"convnet_mnist\"\n",
    "# save (reload using torch.jit.load)\n",
    "torch.jit.save(torch.jit.script(model), MNIST_DIR / f\"{MODEL_NAME}_scripted.pt\")\n",
    "```\n",
    "\n",
    "Or the weights:\n",
    "```python\n",
    "# save (reload using model.load_state_dict, requires the model class!)\n",
    "torch.save(model.state_dict(), MNIST_DIR / f\"{MODEL_NAME}.pt)\n",
    "print(f\"Saved PyTorch Model State to {MNIST_DIR / MODEL_NAME}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ConvNet notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.Conv2d` ([docs](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)) takes as arguments:\n",
    "\n",
    "- `in_channels`: in our case 1, but could be 3 or 4 for colour images.\n",
    "- `out_channels`: how many kernels/filters we want.\n",
    "- `kernel_size`: defines your kernel, aka filter, by specifying the height and width of the matrix 'window' we slide over the image to detect features. Changing these sizes will affect the size of the next layer!\n",
    "- Other arguments include `stride`, `padding`, `padding_mode`...\n",
    "\n",
    "`nn.MaxPool2d` ([docs](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html)) is also a process of sliding through the input, and at each step takes only the maximum value. This is used to downsample!\n",
    "\n",
    "A deep CNN has convolutional layers stacked on top of each other. Each layer is made up of lots of different feature extractors, responding to different kinds of patterns. The output(s) of one layer becomes the input(s) to the next one.\n",
    "\n",
    "- Here the flattening happens using [`view()`](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html), which is efficient and very idiomatic in PyTorch. We flatten the output of the convolutional layers (of shape `(batch_size, channels, w, h)`) to create a single long feature vector `(batch_size, features)`.\n",
    "- [`Dropout`](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout) randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Extra Extra: Training on a custom dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Provided that you have images in a folder like this:\n",
    "```bash\n",
    "main_directory/\n",
    "...class_a/\n",
    "......image_1.jpg\n",
    "......image_2.jpg\n",
    "...class_b/\n",
    "......image_1.jpg\n",
    "......image_2.jpg\n",
    "```\n",
    "\n",
    "You can then replace the data loading by\n",
    "\n",
    "```python\n",
    "# Model / data parameters\n",
    "NUM_CLASSES = # your number of classes\n",
    "\n",
    "transforms = v2.Compose([  \n",
    "    tv.transforms.Grayscale(num_output_channels=1),\n",
    "    tv.transforms.Resize(size=(28,28), antialias=True)\n",
    "])\n",
    "\n",
    "custom_data = tv.datasets.ImageFolder(\n",
    "    DATASETS_DIR / \"custom_dataset\",\n",
    "    transform=transforms,\n",
    ")\n",
    "\n",
    "print(custom_data)\n",
    "print(\"\\n\".join(custom_data.classes)) # should show the folder names\n",
    "\n",
    "train_data, test_data = torch.utils.data.random_split(custom_data, [.9,.1])\n",
    "print(len(train_data), len(test_data))\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "```\n",
    "\n",
    "See [the documentation](https://pytorch.org/vision/main/generated/torchvision.datasets.ImageFolder.html#torchvision.datasets.ImageFolder).\n",
    "\n",
    "Checking the contents, as well as training and testing your net, should be identical as before."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "1c544d3133b9d8c6f36fca025551af31afa9ef134259e7064ad6be0c15e6401c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
